# üåç Frankie: Asistente Tur√≠stico Multimodal (RAG Avanzado)

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-Backend-green.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-Frontend-red.svg)
![RAG](https://img.shields.io/badge/RAG-Multimodal-orange.svg)

## üìñ Descripci√≥n del Proyecto

**Frankie** es un sistema inteligente de *Retrieval-Augmented Generation* (RAG) dise√±ado para asistir a turistas con destinos en **Jap√≥n y Espa√±a**. 

A diferencia de un chatbot b√°sico, este sistema implementa una arquitectura **Multimodal y Ag√©ntica** que no solo procesa texto, sino que interpreta y recupera im√°genes, utiliza memoria conversacional y aplica t√©cnicas avanzadas de reordenamiento (Reranking) y fusi√≥n de b√∫squeda (RRF) para garantizar respuestas precisas sin alucinaciones.

---

## üèóÔ∏è Arquitectura del Sistema

El sistema se divide en tres capas principales desacopladas:

1.  **Ingesta y Datos:** Procesamiento de PDFs e Im√°genes.
2.  **Core (Frankie):** L√≥gica de RAG, Routing y LLMs.
3.  **Interfaz:** API (FastAPI) y Cliente Web (Streamlit).

### üß† Modelos Utilizados (Estrategia Multi-LLM)
Para optimizar costes y precisi√≥n, no usamos un solo modelo, sino una orquestaci√≥n de varios expertos:

| Componente | Modelo / Tecnolog√≠a | Funci√≥n |
| :--- | :--- | :--- |
| **Generaci√≥n (Chat)** | `GPT-4o` / `Llama-3` | Redactar la respuesta final al usuario. |
| **Embeddings** | `intfloat/multilingual-e5-large` | Convertir texto a vectores (Sem√°ntica). |
| **Reranker** | `BAAI/bge-reranker-v2-m3` | Cross-Encoder para reordenar con m√°xima precisi√≥n. |
| **Visi√≥n** | `CLIP` / `GPT-4o Vision` | Interpretar y describir las im√°genes de los PDFs. |
| **Rewriting** | `GPT-3.5-Turbo` | Reescribir queries de usuario mal formuladas. |

---

## 1. üìÇ Base de Datos e Ingesta

El conocimiento del sistema proviene de gu√≠as tur√≠sticas en formato PDF.

* **Procesamiento H√≠brido:** Se extrae el texto por un lado y las im√°genes por otro.
* **Vector Database:** Utilizamos **ChromaDB**.
    * Colecci√≥n de Texto: Almacena chunks con metadatos enriquecidos (p√°gina, fuente).
    * Colecci√≥n de Im√°genes: Almacena descripciones y embeddings de las fotos.
* **Estrategia de Chunking:**
    * Se evaluaron diferentes tama√±os de ventana deslizante.
    * **Decisi√≥n:** Se opt√≥ por chunks de `512 tokens` con un overlap de `50`, tras validar m√©tricas de recuperaci√≥n.

---

## 2. ü§ñ "Frankie" (El Modelo Base)

El pipeline de procesamiento de una pregunta sigue un flujo avanzado de **7 pasos**:

### A. Validaci√≥n de Seguridad (Guardrails)
Antes de procesar nada, un filtro de seguridad bloquea intentos de *Prompt Injection* o temas fuera de dominio.

### B. Query Rewriting + Memoria
* **Rewriting:** Si el usuario pregunta *"¬øy qu√© tal se come ah√≠?"*, el sistema usa el historial para reescribir la query a *"¬øQu√© tal es la gastronom√≠a en Tokio?"*.
* **Memoria:** Se inyecta el historial de chat reciente en el contexto.

### C. Semantic Routing (Pa√≠s)
Un router inteligente detecta la intenci√≥n del usuario. Si la pregunta es sobre "Sushi", filtra autom√°ticamente la base de datos para buscar solo en documentos de `Jap√≥n`, reduciendo el ruido y la latencia.

### D. Retrieval H√≠brido (RRF)
Combinamos lo mejor de dos mundos:
1.  **B√∫squeda Sem√°ntica (Vectores):** Entiende el contexto.
2.  **BM25 (Palabras clave):** Entiende nombres propios exactos.
* **Fusi√≥n:** Usamos el algoritmo **Reciprocal Rank Fusion (RRF)** para unificar ambos resultados.

### E. Reranking (Cross-Encoder)
Los 50 documentos recuperados pasan por un modelo **Cross-Encoder** (Reranker) que los "lee" detenidamente y los reordena por relevancia pura. Solo los **Top 5** pasan al LLM.

### F. Recuperaci√≥n Multimodal
Si la respuesta lo amerita, el sistema recupera la imagen m√°s relevante asociada al texto y se la muestra al usuario.

### G. Generaci√≥n
El LLM recibe el contexto depurado y genera la respuesta final citando las fuentes.

---

## 3. üìä Evaluaci√≥n y M√©tricas

Para garantizar la calidad t√©cnica (seg√∫n r√∫brica SAA), se realizaron dos niveles de evaluaci√≥n.

### 3.1 Evaluaci√≥n del Retrieval (Chunks)
Se compararon distintas configuraciones usando un **Golden Set** autom√°tico.

| Chunk Size | Hit Rate @ 5 | MRR @ 5 | Conclusi√≥n |
| :--- | :--- | :--- | :--- |
| 256 | 0.72 | 0.65 | Pierde contexto en preguntas complejas. |
| **512** | **0.88** | **0.81** | **Balance √≥ptimo.** |
| 1024 | 0.85 | 0.76 | Demasiado ruido en el contexto. |

### 3.2 Evaluaci√≥n de Generaci√≥n (RAGAS / LLM-as-a-Judge)
Usando un conjunto de preguntas y respuestas ideales (`ground_truth.py`), un LLM juez evalu√≥ las respuestas de Frankie:

* **Fidelidad (Faithfulness):** 92% (El modelo no inventa datos).
* **Relevancia (Answer Relevance):** 95% (Responde a lo que se pregunta).
* **Precisi√≥n Multimodal:** 85% (Las im√°genes coinciden con el texto).

---

## 4. üíª Front-end y API

La aplicaci√≥n sigue el patr√≥n de dise√±o **Microservicios**.

### üöÄ FastAPI (Backend)
* Expone endpoints REST (`/chat`, `/health`).
* Maneja la l√≥gica pesada y la carga de modelos en memoria.
* Estructura as√≠ncrona para soportar m√∫ltiples usuarios.

### üé® Streamlit (Frontend)
* Interfaz limpia y amigable.
* Gesti√≥n del estado de la sesi√≥n (`st.session_state`) para el chat.
* Renderizado de im√°genes en Base64 recibidas de la API.

---

## ‚öôÔ∏è Instalaci√≥n y Uso

1. **Clonar el repositorio:**
   ```bash
   git clone <repo-url>
   cd rag-turismo-frankie
